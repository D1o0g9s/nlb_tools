{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://githubtocolab.com/neurallatents/nlb_tools/blob/main/examples/tutorials/gpfa_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPFA Demo\n",
    "\n",
    "While `basic_example.ipynb` used a smoothing implementation to generate rate predictions for the benchmark, this notebook will run GPFA, a better modeling method, using the Python package [`elephant`](https://github.com/NeuralEnsemble/elephant), which should produce far better results. We recommend first viewing `basic_example.ipynb` for more explanation of the `nlb_tools` functions we use here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "\n",
    "Below, we import the necessary functions from `nlb_tools` and additional standard packages. Note that you will need to install `elephant`, which should install with it `neo`, and `quantities` if you don't already have them. Additionally, you'll need `scikit-learn>=0.23` for the Poisson GLM used in this notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "## Install packages if necessary\n",
    "# !pip install elephant\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install git+https://github.com/neurallatents/nlb_tools.git"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## Imports\n",
    "\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import neo\n",
    "import quantities as pq\n",
    "from elephant.gpfa import GPFA\n",
    "from sklearn.linear_model import PoissonRegressor, Ridge"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## If necessary, download dataset from DANDI\n",
    "# !pip install dandi\n",
    "# !dandi install https://dandiarchive.org/dandiset/000138 # replace URL with URL for dataset you want"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Loading data\n",
    "\n",
    "Below, we enter the name of the dataset, the path to the dataset files, as well as a prefix to filter out specific files, in order to load the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## Load dataset\n",
    "\n",
    "dataset_name = 'mc_maze_large'\n",
    "datapath = '~/lvm/code/dandi/000138/sub-Jenkins/'\n",
    "prefix = f'*ses-large'\n",
    "dataset = NWBDataset(datapath, prefix)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "Specified file or directory not found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ce722a418234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'~/lvm/code/dandi/000127/sub-Jenkins/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'*ses-large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNWBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lvm/code/nlb_tools/nlb_tools/nwb_interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fpath, prefix, split_heldout, skip_fields)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Check if file/directory exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Specified file or directory not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# If directory, look for files with matching prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Specified file or directory not found"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Input prep\n",
    "\n",
    "`elephant`'s implementation of GPFA takes its input in the form of lists of `neo.SpikeTrain`s. Here, we'll use `make_train_input_tensor` or `make_eval_input_tensor` to extract the data we want to model before converting it into the desired format."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dataset preparation\n",
    "\n",
    "# Choose the phase here, either 'val' or 'test'\n",
    "phase = 'val'\n",
    "\n",
    "# Choose bin width and resample\n",
    "bin_width = 5\n",
    "dataset.resample(bin_width)\n",
    "\n",
    "# Create suffix for group naming later\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make train input data\n",
    "\n",
    "# Generate input tensors\n",
    "train_trial_split = 'train' if (phase == 'val') else ['train', 'val']\n",
    "train_dict = make_train_input_tensors(dataset, dataset_name=dataset_name, trial_split=train_trial_split, save_file=False)\n",
    "\n",
    "# Unpack input data\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make eval input data\n",
    "\n",
    "# Generate input tensors\n",
    "eval_split = phase\n",
    "eval_dict = make_eval_input_tensors(dataset, dataset_name=dataset_name, trial_split=eval_split, save_file=False)\n",
    "\n",
    "# Unpack data\n",
    "eval_spikes_heldin = eval_dict['eval_spikes_heldin']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Convert spiking array to SpikeTrains\n",
    "\n",
    "def array_to_spiketrains(array):\n",
    "    \"\"\"Converts trial x time x channel spiking arrays to list of list of neo.SpikeTrain\"\"\"\n",
    "    stList = []\n",
    "    # Loop through trials\n",
    "    for trial in range(len(array)):\n",
    "        trialList = []\n",
    "        # Loop through channels\n",
    "        for channel in range(array.shape[2]):\n",
    "            # Get spike times and counts\n",
    "            times = np.where(array[trial, :, channel])[0]\n",
    "            counts = array[trial, times, channel].astype(int)\n",
    "            train = np.repeat(times, counts)\n",
    "            # Create neo.SpikeTrain\n",
    "            st = neo.SpikeTrain(times*bin_width*pq.ms, t_stop=array.shape[1]*bin_width*pq.ms)\n",
    "            trialList.append(st)\n",
    "        stList.append(trialList)\n",
    "    return stList\n",
    "\n",
    "# Run conversion\n",
    "train_st_heldin = array_to_spiketrains(train_spikes_heldin)\n",
    "eval_st_heldin = array_to_spiketrains(eval_spikes_heldin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Running GPFA\n",
    "\n",
    "Now that we have properly formatted data, we'll run GPFA. This step may take quite a while, depending on your machine and the chosen parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Run GPFA\n",
    "\n",
    "# Set parameters\n",
    "bin_size = bin_width * pq.ms\n",
    "latent_dim = 10\n",
    "\n",
    "# Train GPFA on train data and apply on test data\n",
    "gpfa = GPFA(bin_size=bin_size, x_dim=latent_dim)\n",
    "train_factors = gpfa.fit_transform(train_st_heldin)\n",
    "eval_factors = gpfa.transform(eval_st_heldin)\n",
    "\n",
    "# Extract and reshape factors to 3d array\n",
    "train_factors = np.stack([train_factors[i].T for i in range(len(train_factors))])\n",
    "eval_factors = np.stack([eval_factors[i].T for i in range(len(eval_factors))])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Generating rate predictions\n",
    "\n",
    "Now that we have our latent factors at the specified resolution, we can map these factors to the spiking data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Basic data prep\n",
    "\n",
    "# Get input arrays\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']\n",
    "\n",
    "# Assign variables\n",
    "tlength = train_spikes_heldin.shape[1]\n",
    "numtrain = train_spikes_heldin.shape[0]\n",
    "numeval = eval_spikes_heldin.shape[0]\n",
    "numheldin = train_spikes_heldin.shape[2]\n",
    "numheldout = train_spikes_heldout.shape[2]\n",
    "\n",
    "# Reshape data to 2d for regression\n",
    "flatten3d = lambda x: x.reshape(-1, x.shape[2])\n",
    "train_spikes_heldin_s = flatten3d(train_spikes_heldin)\n",
    "train_spikes_heldout_s = flatten3d(train_spikes_heldout)\n",
    "train_factors_s = flatten3d(train_factors)\n",
    "eval_factors_s = flatten3d(eval_factors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Define fitting functions\n",
    "\n",
    "def fit_rectlin(train_input, eval_input, train_output, alpha=0.0):\n",
    "    # Fit linear regression\n",
    "    lr = Ridge(alpha=alpha)\n",
    "    lr.fit(train_input, train_output)\n",
    "    train_pred = lr.predict(train_input)\n",
    "    eval_pred = lr.predict(eval_input)\n",
    "    # Rectify to prevent negative or 0 rate predictions\n",
    "    train_pred[train_pred < 1e-10] = 1e-10\n",
    "    eval_pred[eval_pred < 1e-10] = 1e-10\n",
    "    return train_pred, eval_pred\n",
    "\n",
    "def fit_poisson(train_input, eval_input, train_output, alpha=0.0):\n",
    "    train_pred = []\n",
    "    eval_pred = []\n",
    "    # train Poisson GLM for each output column\n",
    "    for chan in range(train_output.shape[1]):\n",
    "        pr = PoissonRegressor(alpha=alpha, max_iter=500)\n",
    "        pr.fit(train_input, train_output[:, chan])\n",
    "        train_pred.append(pr.predict(train_input))\n",
    "        eval_pred.append(pr.predict(eval_input))\n",
    "    train_pred = np.vstack(train_pred).T\n",
    "    eval_pred = np.vstack(eval_pred).T\n",
    "    return train_pred, eval_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make rate predictions\n",
    "\n",
    "# fit GLMs for rate predictions\n",
    "train_rates_heldin_s, eval_rates_heldin_s = fit_rectlin(train_factors_s, eval_factors_s, train_spikes_heldin_s)\n",
    "train_rates_heldout_s, eval_rates_heldout_s = fit_poisson(train_rates_heldin_s, eval_rates_heldin_s, train_spikes_heldout_s)\n",
    "\n",
    "# reshape output back to 3d\n",
    "train_rates_heldin = train_rates_heldin_s.reshape((numtrain, tlength, numheldin))\n",
    "train_rates_heldout = train_rates_heldout_s.reshape((numtrain, tlength, numheldout))\n",
    "eval_rates_heldin = eval_rates_heldin_s.reshape((numeval, tlength, numheldin))\n",
    "eval_rates_heldout = eval_rates_heldout_s.reshape((numeval, tlength, numheldout))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Making the submission\n",
    "\n",
    "Now, we'll make the submission dict manually. As described in `basic_example.ipynb`, you can also use the function `save_to_h5` from `make_tensors.py` to save the output as an h5 file for submission on EvalAI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Prepare submission data\n",
    "\n",
    "output_dict = {\n",
    "    dataset_name + suffix: {\n",
    "        'train_rates_heldin': train_rates_heldin,\n",
    "        'train_rates_heldout': train_rates_heldout,\n",
    "        'eval_rates_heldin': eval_rates_heldin,\n",
    "        'eval_rates_heldout': eval_rates_heldout\n",
    "    }\n",
    "}\n",
    "\n",
    "# To save as an h5 file:\n",
    "# save_to_h5(output_dict, 'submission.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Finally, we will create the target data with `make_eval_target_tensors` and evaluate our model if we ran on the 'val' phase. If the notebook was run on the 'test' phase, you would need to submit to the EvalAI challenge to get results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make data to test predictions with and evaluate\n",
    "\n",
    "if phase == 'val':\n",
    "    target_dict = make_eval_target_tensors(dataset, dataset_name=dataset_name, train_trial_split='train', eval_trial_split='val', include_psth=('mc_rtt' not in dataset_name), save_file=False)\n",
    "\n",
    "    print(evaluate(target_dict, output_dict))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'area2_bump_split': {'coNLL': 3.438209105335957, 'vel R2': 0.5704200272213656, 'psth R2': 0.476563983978655}}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we used `nlb_tools` and `elephant` to run GPFA on a dataset for the Neural Latents Benchmark."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('tf2-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
   }
  },
  "interpreter": {
   "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}