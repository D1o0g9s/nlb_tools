{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('tf2-gpu': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
   }
  },
  "interpreter": {
   "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SLDS Demo\n",
    "\n",
    "In this notebook, we will use a switching linear dynamical system (SLDS) to model the neural data. We will use the Linderman Lab's [`ssm` package](https://github.com/lindermanlab/ssm), which you should install before running this demo. We recommend first viewing `basic_example.ipynb` for more explanation of the `nlb_tools` functions we use here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "\n",
    "Below, we import the necessary functions from `nlb_tools` and additional standard packages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "## Install packages if necessary\n",
    "# !pip install git+https://github.com/lindermanlab/ssm\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install git+https://github.com/neurallatents/nlb_tools.git"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## Imports\n",
    "\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "\n",
    "import ssm\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## If necessary, download dataset from DANDI\n",
    "# !pip install dandi\n",
    "# !dandi install https://dandiarchive.org/dandiset/000138 # replace URL with URL for dataset you want"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Loading data\n",
    "\n",
    "Below, please enter the path to the dataset, as well as the name of the dataset, to load the data. In addition, you can choose a bin size (0.005 or 0.02 s) to run the notebook at."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## Load dataset\n",
    "\n",
    "dataset_name = 'mc_maze_small'\n",
    "datapath = '~/lvm/code/dandi/000128/sub-Jenkins/'\n",
    "prefix = f'*ses-small'\n",
    "dataset = NWBDataset(datapath, prefix)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "Specified file or directory not found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d61b6ce95d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'~/data/000128/sub-Jenkins/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'*ses-small'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNWBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lvm/code/nlb_tools/nlb_tools/nwb_interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fpath, prefix, split_heldout, skip_fields)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Check if file/directory exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Specified file or directory not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# If directory, look for files with matching prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Specified file or directory not found"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Input prep\n",
    "\n",
    "`ssm` expects inputs as a list of 2d arrays of type `int`, so we will use functions from `make_tensors` to create 3d arrays, and split the arrays along the trial axis to get our list."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dataset preparation\n",
    "\n",
    "# Choose the phase here, either 'val' or 'test'\n",
    "phase = 'val'\n",
    "\n",
    "# Choose bin width and resample\n",
    "bin_width = 5\n",
    "dataset.resample(bin_width)\n",
    "\n",
    "# Create suffix for group naming later\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make train input data\n",
    "\n",
    "# Generate input tensors\n",
    "train_trial_split = 'train' if (phase == 'val') else ['train', 'val']\n",
    "train_dict = make_train_input_tensors(dataset, dataset_name=dataset_name, trial_split=train_trial_split, save_file=False, include_forward_pred=True)\n",
    "\n",
    "# Unpack input data\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make eval input data\n",
    "\n",
    "# Generate input tensors\n",
    "eval_trial_split = phase\n",
    "eval_dict = make_eval_input_tensors(dataset, dataset_name=dataset_name, trial_split=eval_trial_split, save_file=False)\n",
    "\n",
    "# Unpack data\n",
    "eval_spikes_heldin = eval_dict['eval_spikes_heldin']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Prep input\n",
    "\n",
    "# Combine train spiking data into one array\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']\n",
    "train_spikes_heldin_fp = train_dict['train_spikes_heldin_forward']\n",
    "train_spikes_heldout_fp = train_dict['train_spikes_heldout_forward']\n",
    "train_spikes = np.concatenate([\n",
    "    np.concatenate([train_spikes_heldin, train_spikes_heldin_fp], axis=1),\n",
    "    np.concatenate([train_spikes_heldout, train_spikes_heldout_fp], axis=1),\n",
    "], axis=2)\n",
    "\n",
    "# Fill missing test spiking data with zeros and make masks\n",
    "eval_spikes_heldin = eval_dict['eval_spikes_heldin']\n",
    "eval_spikes = np.full((eval_spikes_heldin.shape[0], train_spikes.shape[1], train_spikes.shape[2]), 0.0)\n",
    "masks = np.full((eval_spikes_heldin.shape[0], train_spikes.shape[1], train_spikes.shape[2]), False)\n",
    "eval_spikes[:, :eval_spikes_heldin.shape[1], :eval_spikes_heldin.shape[2]] = eval_spikes_heldin\n",
    "masks[:, :eval_spikes_heldin.shape[1], :eval_spikes_heldin.shape[2]] = True\n",
    "\n",
    "# Make lists of arrays\n",
    "train_datas = [train_spikes[i, :, :].astype(int) for i in range(len(train_spikes))]\n",
    "eval_datas = [eval_spikes[i, :, :].astype(int) for i in range(len(eval_spikes))]\n",
    "eval_masks = [masks[i, :, :].astype(bool) for i in range(len(masks))]\n",
    "\n",
    "num_heldin = train_spikes_heldin.shape[2]\n",
    "tlen = train_spikes_heldin.shape[1]\n",
    "num_train = len(train_datas)\n",
    "num_eval = len(eval_datas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Running SLDS\n",
    "\n",
    "Now that we have our input data prepared, we can fit an SLDS to it. Feel free to vary the parameters as you see fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Run SLDS\n",
    "\n",
    "# Set parameters\n",
    "T = train_datas[0].shape[0] # trial length\n",
    "K = 5 # number of discrete states\n",
    "D = 15 # dimensionality of latent states\n",
    "N = train_datas[0].shape[1] # input dimensionality\n",
    "\n",
    "slds = ssm.SLDS(N, K, D,\n",
    "    transitions='standard',\n",
    "    emissions='poisson',\n",
    "    emission_kwargs=dict(link=\"log\"),\n",
    "    dynamics_kwargs={\n",
    "        'l2_penalty_A': 3000.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train\n",
    "q_elbos_lem_train, q_lem_train = slds.fit(\n",
    "    datas=train_datas,\n",
    "    method=\"laplace_em\",\n",
    "    variational_posterior=\"structured_meanfield\",\n",
    "    num_init_iters=25, num_iters=25, alpha=0.2,\n",
    ")\n",
    "\n",
    "# Pass eval data\n",
    "q_elbos_lem_eval, q_lem_eval = slds.approximate_posterior(\n",
    "    datas=eval_datas,\n",
    "    masks=eval_masks,\n",
    "    method=\"laplace_em\",\n",
    "    variational_posterior=\"structured_meanfield\",\n",
    "    num_iters=25, alpha=0.2,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca7de7a061914ec3adbf09067b8708be"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing with an ARHMM using 25 steps of EM.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f0eb9c62e3349fa801879b031f587c8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0aa1ae4dc14d46648e01f97be9df16a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26db4a13e42d4d8f940735bce295d530"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Generating rate predictions\n",
    "\n",
    "We now have our estimates of continuous neural population state, so we'll now use them to predict neuron firing rates. `SLDS` does this by smoothing the input data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Generate rate predictions\n",
    "\n",
    "# Smooth observations using inferred states\n",
    "train_rates = [slds.smooth(q_lem_train.mean_continuous_states[i], train_datas[i]) for i in range(num_train)]\n",
    "eval_rates = [slds.smooth(q_lem_eval.mean_continuous_states[i], eval_datas[i], mask=eval_masks[i]) for i in range(num_eval)]\n",
    "\n",
    "# Reshape output\n",
    "train_rates = np.stack(train_rates)\n",
    "eval_rates = np.stack(eval_rates)\n",
    "\n",
    "train_rates_heldin = train_rates[:, :tlen, :num_heldin]\n",
    "train_rates_heldout = train_rates[:, :tlen, num_heldin:]\n",
    "eval_rates_heldin = eval_rates[:, :tlen, :num_heldin]\n",
    "eval_rates_heldout = eval_rates[:, :tlen, num_heldin:]\n",
    "eval_rates_heldin_forward = eval_rates[:, tlen:, :num_heldin]\n",
    "eval_rates_heldout_forward = eval_rates[:, tlen:, num_heldin:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Making the submission\n",
    "\n",
    "Now, we'll make the submission dict manually. As described in `basic_example.ipynb`, you can also use the function `save_to_h5` from `make_tensors.py` to save the output as an h5 file for submission on EvalAI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Prepare submission data\n",
    "\n",
    "output_dict = {\n",
    "    dataset_name + suffix: {\n",
    "        'train_rates_heldin': train_rates_heldin,\n",
    "        'train_rates_heldout': train_rates_heldout,\n",
    "        'eval_rates_heldin': eval_rates_heldin,\n",
    "        'eval_rates_heldout': eval_rates_heldout,\n",
    "        'eval_rates_heldin_forward': eval_rates_heldin_forward,\n",
    "        'eval_rates_heldout_forward': eval_rates_heldout_forward,\n",
    "    }\n",
    "}\n",
    "\n",
    "# To save as an h5 file:\n",
    "# save_to_h5(output_dict, 'submission.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Finally, we will create the test data with make_test_tensor and evaluate our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Make data to test predictions with and evaluate\n",
    "\n",
    "if phase == 'val':\n",
    "    target_dict = make_eval_target_tensors(dataset, dataset_name=dataset_name, train_trial_split='train', eval_trial_split='val', include_psth=('mc_rtt' not in dataset_name), save_file=False)\n",
    "\n",
    "    print(evaluate(target_dict, output_dict))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'area2_bump_split': {'coNLL': 3.4609330751624525, 'vel R2': 0.4460491722640292, 'psth R2': 0.3652557278345789, 'fpNLL': 3.5617343127606995}}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we used `nlb_tools` and `ssm` to run and evaluate SLDS on our benchmark."
   ],
   "metadata": {}
  }
 ]
}